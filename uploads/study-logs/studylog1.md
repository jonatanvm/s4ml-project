# Study log 1
## What does it mean R3E for YOUR big data and machine learning systems?

When making trade decisions using machine learning it is extremely important to have robustness, so that errors do not lead to unreasonable trades. Data can be hard and expensive to acquire, but financial data fetched from the apis of large organisations can for the most part be considered accurate and reliable. Overfitting and underfitting can be tested for using historical data, but anomalies could still happen which could compromise the algorithm. It could thus be wise to set intervals for the maximum and minimum value of the financial transactions done, to reduce the effect of anomalies which could cause a loss of a lot of money.  

If you are using apis to fetch data from a trading ml algorithm, it is however important to make sure the data you are fetching is actually from the sender you think it is, and that the data isn't altered when it is being sent to you. Thankfully there are however well established end-to-end encrypted web protocols, which can and are used to mitigate this problem. Many of the implementations of the apis for specific programming languages are however open-source, which could lead to attackers being able to push malicious code into the api repositories. One way to mitigate this problem would be to fork a version of the repository which you had made sure did not contain any malicious code and manually review and apply all the commits that you had made sure didn't contain any malicious code. This however would add a massive code review overhead to the maintaining of the software.  

Elasticity could be kept by doing the computations in the cloud where platforms such as Google CLoud Platform and AWS, allow for dynamically changing the RAM, CPU core count and CPU core speed. Cloud computation however would expose the system to additional security threats. Dynamically changing the algorithm would naturally be made by first extensively testing the algorithm offline, and then deploying it once addacuate results where observed. As financial data flows from apis can be predictable and the computational requirements of the algorithm could be tested extensively before deploying the algorithm, elasticity in computational requirements shouldn't be a big problem. One natural elasticity in a machine learning trade system would however be to be able to adjust the riskiness of the trades, to be able to fit the needs of many different investors.  

### In your experience/work, which ones of R3E concern you most? Why? What would you do? What do you look for?

I think that reliability is one of the hardest problems to solve i.e. getting and keeping an ml-algorithm working exactly as needed and specified. Performance has also many times have been a problem but this could quite easily be solved by cloud computing. Thus far I've really never had to think about system attacks and bugs, and have mostly built my machine learning algorithms on the assumption that the source code works and doesn't have major bugs. I've also mostly worked with fixed data sets so resilience in the data has not really been a problem. Overfitting and underfitting problems have occurred quite frequently but haven't really presented a huge problem because I've had a lot of time to fix them. Most of the established methods to test the robustness of an ml algorithm has thus far worked well in my projects. Data sanitation and feature extraction has also proved to be a big problem many times.
